<p align="center">
  <h1>ğŸ“˜ Review Buku</h1>
  <img src="image.png" width="300"/>
  <h3><u>Introduction to Humanoid Robotics</u></h3>
</p>

---

## BAB 1 â€“ Introduction (Robot Humanoid)

### ğŸ¤– Apa itu Robot Humanoid?
Robot dengan bentuk & gerak menyerupai manusia:
- 2 kaki, 2 tangan, badan, kepala

**Kenapa bentuk manusia?**
- Bekerja di lingkungan manusia
- Menggunakan alat manusia
- Interaksi sosial lebih alami

ğŸ“Œ Robot menyesuaikan diri dengan dunia manusia â€” bukan sebaliknya

---

## BAB 2 â€“ Kinematics (Gerak Robot)

ğŸ“ Kinematika = ilmu gerak **tanpa mempedulikan gaya**

Digunakan untuk:
- Menghitung posisi tangan/kaki
- Menentukan sudut sendi
- Menghubungkan bagian tubuh

### Konsep Kunci:
- Coordinate Frame
- Homogeneous Transformation Matrix
- Forward & Inverse Kinematics
- Jacobian & Singularitas

---

## BAB 3 â€“ ZMP & Dynamics (Keseimbangan)

âš ï¸ Tantangan utama humanoid: **tidak jatuh**

### Zero Moment Point (ZMP)
- Titik di lantai tempat momen = 0
- **ZMP harus berada di dalam Support Polygon**

Digunakan bersama:
- Center of Mass
- Force/Torque Sensor
- Dinamika tubuh

---

## BAB 4 â€“ Biped Walking (Berjalan Dua Kaki)

Berjalan â‰  mengangkat kaki.

Robot harus:
- Memindahkan CoM
- Menjaga keseimbangan
- Menghindari slip

ğŸ“Œ Model utama:
- **Linear Inverted Pendulum Model (LIPM)**
- **ZMP-Based Walking**
- **Preview Control**

---

## BAB 5 â€“ Whole Body Motion

Robot mampu:
- Berdiri dari jatuh
- Membawa barang
- Membuka pintu
- Jatuh dengan aman

Teknik penting:
- Motion Capture
- Auto Balancer
- Dynamics Filter

---

## BAB 6 â€“ Dynamic Simulation

Semua diuji di simulasi sebelum ke robot nyata.

Metode utama:
- Forward & Inverse Dynamics
- Newtonâ€“Euler
- Featherstone Algorithm

ğŸ“Œ Digunakan di:
- ROS
- Gazebo
- MuJoCo

---

## ğŸ“š Kosakata Penting (Cheat Sheet)

| No | Istilah | Arti Singkat | Inti |
|--|--|--|--|
| 1 | Joint | Sendi bergerak | Tempat gerak |
| 2 | Link | Bagian kaku | Tidak bergerak |
| 3 | DOF | Derajat kebebasan | Fleksibilitas |
| 4 | FK | Sudut â†’ posisi | Mudah |
| 5 | IK | Posisi â†’ sudut | Sulit |
| 6 | Jacobian | Kecepatan | Kontrol |
| 7 | ZMP | Stabilitas | Tidak jatuh |
| 8 | CoM | Pusat massa | Keseimbangan |
| 9 | LIPM | Model berjalan | Sederhana |
| 10 | Simulation | Uji virtual | Aman |

<br><br><br>
<p align="center">
  <h1>ğŸš€ Review Video</h1>
  <img src="image-1.png" width="600"/>
  <h3><u>The FASTEST Introduction to Reinforcement Learning on the Internet</u></h3>
</p>

---

## ğŸ¯ Apa itu Reinforcement Learning (RL)?

**Reinforcement Learning (RL)** adalah paradigma *machine learning* di mana sebuah **agen** belajar melalui **trial and error** dengan berinteraksi langsung dengan lingkungannya.

Alih-alih diberi jawaban benar/salah, agen:
> melakukan aksi â†’ menerima feedback â†’ memperbaiki strategi



### ğŸ”‘ Komponen Utama RL
1. **State ($S$)** â€“ Kondisi lingkungan saat ini (posisi robot, sensor, kecepatan, koordinat sendi, dll).
2. **Action ($A$)** â€“ Tindakan yang dapat diambil agen (torsi motor, langkah kaki, rotasi servo).
3. **Reward ($R$)** â€“ Nilai evaluasi (positif jika stabil/mencapai goal, negatif jika jatuh/tabrakan).
4. **Policy ($\pi$)** â€“ Strategi atau "peta" untuk memilih action terbaik pada setiap state.

ğŸ¯ **Tujuan RL:** Memaksimalkan **total reward jangka panjang** (akumulasi imbalan hingga akhir tugas).

---

## ğŸ”„ Bagaimana Reinforcement Learning Bekerja? (Detail Teknis)

Proses RL berjalan secara **iteratif** melalui mekanisme **Markov Decision Process (MDP)**:

1. **Observasi** â€“ Agen mengamati **State** (berdasarkan hukum *Markov Property*: masa lalu tidak penting, hanya kondisi sekarang yang menentukan masa depan).
2. **Aksi** â€“ Agen memilih **Action** berdasarkan **Policy** saat ini.
3. **Reward** â€“ Lingkungan memberi feedback yang dikalikan dengan **Discount Factor ($\gamma$)** agar agen tidak menunda-nunda tujuan.
4. **Update Policy** â€“ Strategi diperbarui menggunakan **Bellman Equation** untuk menghitung nilai masa depan.
5. **Repeat** â€“ Dilakukan ribuan hingga jutaan kali hingga konvergen.

---

## ğŸ® Contoh Sederhana (Analogi Game & Simulasi)

- Mulai dari level awal (**state**)
- Pilih langkah (**action**)
- Dapat skor / penalti (**reward**)
- Belajar strategi terbaik (**policy**)

â¡ï¸ Semakin sering bermain, semakin pintar strateginya melalui pembaruan **Q-Table** atau **Neural Network**.

---

## ğŸ¤– Mengapa RL Penting untuk Robot Humanoid?

Robot humanoid **tidak mungkin diprogram secara manual untuk semua kondisi**, karena masalah yang dijelaskan dalam buku *Introduction to Humanoid Robotics*:

* **Lingkungan Dinamis:** Medan yang tidak rata atau licin sulit dihitung secara kaku.
* **Derajat Kebebasan (DoF) yang Tinggi:** Terlalu banyak sendi untuk dikontrol manual satu per satu.
* **Keseimbangan ZMP:** Menjaga *Zero Moment Point* (Bab 3) membutuhkan koreksi mikro instan.

ğŸ”‘ **RL memungkinkan robot:**
- **Belajar dari pengalaman:** Menemukan pola jalan yang stabil secara mandiri.
- **Beradaptasi:** Menangani gangguan fisik (dorongan atau permukaan miring) secara *real-time*.
- **Optimasi Kinematika:** Menyelesaikan masalah *Inverse Kinematics* (Bab 2) melalui intuisi data, bukan sekadar rumus statis.



---

## ğŸ§  Algoritma Reinforcement Learning & Koneksi Biologis

| Metode | Penjelasan Singkat | Hubungan dengan Otak |
|------|-------------------|-------------------|
| **Value-Based** | Belajar nilai setiap aksi (Contoh: Q-Learning / DQN) | Memori tentang mana yang "berharga". |
| **Policy-Based** | Langsung belajar strategi tindakan terbaik | Insting motorik. |
| **Actorâ€“Critic** | Gabungan value (Critic) & policy (Actor) | **Basal Ganglia** (Striatum sebagai Critic). |
| **Deep RL** | RL + Neural Network (untuk sensor kompleks) | Cara saraf manusia memproses visual/IMU. |

> **Fakta Penting:** RL meniru sistem **Dopamin** manusia sebagai *Reward Prediction Error* (RPE). Belajar terjadi ketika robot "terkejut" karena hasil nyata lebih baik dari prediksi awal.

---

## ğŸ§¾ Ringkasan Parameter RL

| Konsep | Makna | Aplikasi pada Humanoid |
|------|------|------|
| **RL** | Belajar dari trial-and-error | Optimasi pola jalan (Walking Pattern). |
| **Agent** | Robot / algoritma | Controller (Open-HRP / Matlab). |
| **State** | Kondisi lingkungan | Sensor IMU & Posisi ZMP. |
| **Action** | Tindakan agen | Torsi motor pada sendi bipedal. |
| **Reward** | Feedback | +1 jika melangkah, -100 jika jatuh. |
| **Policy** | Strategi | Algoritma penyeimbang (Stabilizer). |
| **Goal** | Maksimalkan reward | Navigasi tanpa jatuh. |

<br>

## ğŸš€ Tantangan: Sample Inefficiency
Karena robot butuh jutaan percobaan, pembuatan humanoid harus melibatkan:
1. **Simulasi Dinamis (Bab 6):** Latihan di dunia maya (Gazebo/MuJoCo) agar hardware tidak rusak.
2. **World Models:** Agen membangun imajinasi tentang hukum fisika sebelum mencoba aksi fisik.

<hr/>


## ğŸ§  Kesimpulan

ğŸ“˜ **Buku** â†’ fondasi matematis & kontrol humanoid  
ğŸ® **RL** â†’ kecerdasan & adaptasi perilaku  

â¡ï¸ **RL berdiri di atas kinematika, dinamika, dan ZMP**  

---
